sigma_0_sq = 1/10
beta_0 = c(0,0)
# Prior predictive distribution
SIGMA_SQ = 1/rgamma(1,nu_0/2,nu_0*sigma_0_sq/2)
Sigma_0 = g*SIGMA_SQ*solve((t(X)%*%X))
BETA = rmvnorm(1, beta_0, Sigma_0)
X
BETA
Y <- read.table("http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/swim.dat")
Y <- t(Y)
# data summaries
n = nrow(Y)
n_swimmers = ncol(Y)
g = n
W = seq(2,12,length.out=n)
X = cbind(rep(1,n),(W-mean(W)))
p = ncol(X)
# Hyperparameters for the prior
nu_0 = 1
sigma_0_sq = 1/10
beta_0 = c(0,0)
n_iter = 1000
prior_predict = matrix(0, nrow=n_iter, ncol=nrow(X))
for (i in 1:n_iter){
# Prior predictive distribution
SIGMA_SQ = 1/rgamma(1,nu_0/2,nu_0*sigma_0_sq/2)
Sigma_0 = g*SIGMA_SQ*solve((t(X)%*%X))
BETA = rmvnorm(1, beta_0, Sigma_0)
prior_predict[i,] = rmvnorm(1, X%*%t(BETA), SIGMA_SQ*diag(1,nrow=n)) # save different weeks
}
colnames(prior_predict) = c('Week 2','Week 4','Week 6','Week 8','Week 10','Week 12')
plot(density(prior_predict[,'Week 2']),col="red3",xlim=c(-3,3),ylim=c(0,3),lwd=1.5,
main="Prior Predictive Distributions",xlab="swimming times")
legend("topright",2,c('Week 2','Week 4','Week 6','Week 8','Week 10','Week 12'),col=c("red3","blue3","orange2","black","green","purple"),lwd=2,bty="n")
lines(density(prior_predict[,"Week 4"]),col="blue3",lwd=1.5)
lines(density(prior_predict[,"Week 6"]),col="orange2",lwd=1.5)
lines(density(prior_predict[,"Week 8"]),col="black",lwd=1.5)
lines(density(prior_predict[,"Week 10"]),col="green",lwd=1.5)
lines(density(prior_predict[,"Week 12"]),col="purple",lwd=1.5)
View(prior_predict)
plot(density(prior_predict[,'Week 2']),col="red3",xlim=c(-3,3),ylim=c(0,1.7),lwd=1.5,
main="Prior Predictive Distributions",xlab="swimming times")
legend("topright",2,c('Week 2','Week 4','Week 6','Week 8','Week 10','Week 12'),col=c("red3","blue3","orange2","black","green","purple"),lwd=2,bty="n")
lines(density(prior_predict[,"Week 4"]),col="blue3",lwd=1.5)
lines(density(prior_predict[,"Week 6"]),col="orange2",lwd=1.5)
lines(density(prior_predict[,"Week 8"]),col="black",lwd=1.5)
lines(density(prior_predict[,"Week 10"]),col="green",lwd=1.5)
lines(density(prior_predict[,"Week 12"]),col="purple",lwd=1.5)
# Hyperparameters for the prior
beta_ols <- solve(t(X)%*%X)%*%t(X)%*%Y
# Hyperparameters for the prior
beta_ols <- solve(t(X)%*%X)%*%t(X)%*%Y
# set number of iterations
S <- 10000
BETA = array(0,c(n_swimmers, S, p))
SIGMA_SQ = matrix(0,n_swimmers,S)
for (j in 1:n_swimmers){
# sample sigma_sq
nu_n <- nu_0 + n
Hg <- (g/(g+1))* X%*%solve(t(X)%*%X)%*%t(X)
SSRg <- t(Y[,j])%*%(diag(1,nrow=n) - Hg)%*%Y[,j]
nu_n_sigma_n_sq <- nu_0*sigma_0_sq + SSRg
sigma_sq <- 1/rgamma(S,(nu_n/2),(nu_n_sigma_n_sq/2))
# sample beta
mu_n <- g*beta_ols[,j]/(g+1)
beta <- matrix(nrow=S,ncol=p)
for(s in 1:S){
Sigma_n <- g*sigma_sq[s]*solve(t(X)%*%X)/(g+1)
beta[s,] <- rmvnorm(1,mu_n,Sigma_n)
}
BETA[j,,] = beta
SIGMA_SQ[j,] = sigma_sq
}
# posterior summaries
beta_postmean <- t(apply(BETA,c(1,3),mean))
colnames(beta_postmean) <- c("Swimmer 1","Swimmer 2","Swimmer 3","Swimmer 4")
rownames(beta_postmean) <- c("beta_0","beta_1")
beta_postmean
dim(BETA)
Y <- read.table("http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/swim.dat")
Y <- t(Y)
# data summaries
n = nrow(Y)
n_swimmers = ncol(Y)
g = n
W = seq(2,12,length.out=n)
X = cbind(rep(1,n),(W-mean(W)))
p = ncol(X)
# Hyperparameters for the prior
nu_0 = 1
sigma_0_sq = 1/10
beta_0 = c(0,0)
# Hyperparameters for the prior
beta_ols <- solve(t(X)%*%X)%*%t(X)%*%Y
# set number of iterations
S <- 10000
BETA = array(0,c(n_swimmers, S, p))
SIGMA_SQ = matrix(0,n_swimmers,S)
# sample sigma_sq
nu_n <- nu_0 + n
Hg <- (g/(g+1))* X%*%solve(t(X)%*%X)%*%t(X)
SSRg <- t(Y)%*%(diag(1,nrow=n) - Hg)%*%Y
nu_n_sigma_n_sq <- nu_0*sigma_0_sq + SSRg
sigma_sq <- 1/rgamma(S,(nu_n/2),(nu_n_sigma_n_sq/2))
# sample beta
mu_n <- g*beta_ols/(g+1)
beta <- matrix(nrow=S,ncol=p)
for(s in 1:S){
Sigma_n <- g*sigma_sq[s]*solve(t(X)%*%X)/(g+1)
beta[s,] <- rmvnorm(1,mu_n,Sigma_n)
}
dim(mu_n)
Y <- read.table("http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/swim.dat")
Y <- t(Y)
# data summaries
n = nrow(Y)
n_swimmers = ncol(Y)
g = n
W = seq(2,12,length.out=n)
X = cbind(rep(1,n),(W-mean(W)))
p = ncol(X)
# Hyperparameters for the prior
nu_0 = 1
sigma_0_sq = 1/10
beta_0 = c(0,0)
View(Y)
# set number of iterations
S <- 10000
BETA = array(0,c(n_swimmers, S, p))
SIGMA_SQ = matrix(0,n_swimmers,S)
for (j in 1:n_swimmers){
beta_ols <- solve(t(X)%*%X)%*%t(X)%*%Y[,j]
# sample sigma_sq
nu_n <- nu_0 + n
Hg <- (g/(g+1))* X%*%solve(t(X)%*%X)%*%t(X)
SSRg <- t(Y[,j])%*%(diag(1,nrow=n) - Hg)%*%Y[,j]
nu_n_sigma_n_sq <- nu_0*sigma_0_sq + SSRg
sigma_sq <- 1/rgamma(S,(nu_n/2),(nu_n_sigma_n_sq/2))
# sample beta
mu_n <- g*beta_ols[,j]/(g+1)
beta <- matrix(nrow=S,ncol=p)
for(s in 1:S){
Sigma_n <- g*sigma_sq[s]*solve(t(X)%*%X)/(g+1)
beta[s,] <- rmvnorm(1,mu_n,Sigma_n)
}
BETA[j,,] = beta
SIGMA_SQ[j,] = sigma_sq
}
Y <- read.table("http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/swim.dat")
Y <- t(Y)
# data summaries
n = nrow(Y)
n_swimmers = ncol(Y)
g = n
W = seq(2,12,length.out=n)
X = cbind(rep(1,n),(W-mean(W)))
p = ncol(X)
# Hyperparameters for the prior
nu_0 = 1
sigma_0_sq = 1/10
beta_0 = c(0,0)
# set number of iterations
S <- 10000
BETA = array(0,c(n_swimmers, S, p))
SIGMA_SQ = matrix(0,n_swimmers,S)
for (j in 1:n_swimmers){
beta_ols <- solve(t(X)%*%X)%*%t(X)%*%Y[,j]
# sample sigma_sq
nu_n <- nu_0 + n
Hg <- (g/(g+1))* X%*%solve(t(X)%*%X)%*%t(X)
SSRg <- t(Y[,j])%*%(diag(1,nrow=n) - Hg)%*%Y[,j]
nu_n_sigma_n_sq <- nu_0*sigma_0_sq + SSRg
sigma_sq <- 1/rgamma(S,(nu_n/2),(nu_n_sigma_n_sq/2))
# sample beta
mu_n <- g*beta_ols/(g+1)
beta <- matrix(nrow=S,ncol=p)
for(s in 1:S){
Sigma_n <- g*sigma_sq[s]*solve(t(X)%*%X)/(g+1)
beta[s,] <- rmvnorm(1,mu_n,Sigma_n)
}
BETA[j,,] = beta
SIGMA_SQ[j,] = sigma_sq
}
# posterior summaries
beta_postmean <- t(apply(BETA,c(1,3),mean))
colnames(beta_postmean) <- c("Swimmer 1","Swimmer 2","Swimmer 3","Swimmer 4")
rownames(beta_postmean) <- c("beta_0","beta_1")
beta_postmean
## (c) For each swimmer j, plot their posterior predictive distributions for a future time T* two weeks after the last recorded observations
x_new <- matrix(c(1,(14-mean(W))),ncol=1)
post_pred <- matrix(0,nrow=n_iter,ncol=n_swimmers)
n_iter = 1000
post_pred <- matrix(0,nrow=n_iter,ncol=n_swimmers)
for(j in 1:n_swimmers){
post_pred[,j] <- rnorm(n_iter,BETA[j,,]%*%x_new,sqrt(SIGMA_SQ[j,]))
}
colnames(post_pred) <- c("Swimmer 1","Swimmer 2","Swimmer 3","Swimmer 4")
plot(density(post_pred[,"Swimmer 1"]),col="red3",xlim=c(0,30),ylim=c(0,0.5),lwd=1.5,
main="Predictive Distributions",xlab="swimming times")
legend("topleft",2,c("Swimmer1","Swimmer2","Swimmer3","Swimmer4"),col=c("red3","blue3","orange2","black"),lwd=2,bty="n")
lines(density(post_pred[,"Swimmer 2"]),col="blue3",lwd=1.5)
lines(density(post_pred[,"Swimmer 3"]),col="orange2",lwd=1.5)
lines(density(post_pred[,"Swimmer 4"]),lwd=1.5)
az = read.table("azdiabetes.dat.txt", header = TRUE)[,-8]
plot(density(post_pred[,"Swimmer 1"]),col="red3",xlim=c(-100,100),ylim=c(0,0.05),lwd=1.5,
main="Predictive Distributions",xlab="swimming times")
legend("topleft",2,c("Swimmer1","Swimmer2","Swimmer3","Swimmer4"),col=c("red3","blue3","orange2","black"),lwd=2,bty="n")
lines(density(post_pred[,"Swimmer 2"]),col="blue3",lwd=1.5)
lines(density(post_pred[,"Swimmer 3"]),col="orange2",lwd=1.5)
lines(density(post_pred[,"Swimmer 4"]),lwd=1.5)
## (d) compute P(Yj = max(Y1, Y2, Y3, Y4)) for each swimmer j, and based on this make a recommendation to the coach
post_pred_min <- as.data.frame(apply(post_pred,1,function(x) which(x==min(x))))
colnames(post_pred_min) <- "Swimmers"
post_pred_min$Swimmers <- as.factor(post_pred_min$Swimmers)
levels(post_pred_min$Swimmers) <- c("Swimmer 1","Swimmer 2","Swimmer 3","Swimmer 4")
table(post_pred_min$Swimmers)/n_iter
## (d) compute P(Yj = max(Y1, Y2, Y3, Y4)) for each swimmer j, and based on this make a recommendation to the coach
post_pred_min <- as.data.frame(apply(post_pred,1,function(x) which(x==max(x))))
colnames(post_pred_min) <- "Swimmers"
post_pred_min$Swimmers <- as.factor(post_pred_min$Swimmers)
levels(post_pred_min$Swimmers) <- c("Swimmer 1","Swimmer 2","Swimmer 3","Swimmer 4")
table(post_pred_min$Swimmers)/n_iter
az = read.table("azdiabetes.dat.txt", header = TRUE)[,-8]
View(az)
# glu ~ 1 + npreg + bp + skin + bmi + ped + age
### data and priors
n = nrow(az)
intercept = as.matrix(rep(1, n), ncol = 1)
colnames(intercept) = c("intercept")
rownames(intercept) = 1:nrow(az)
X = cbind(intercept, as.matrix(az[, -2])) #delete glu col
Y = as.matrix(az[, 2], ncol = 1)
p = ncol(X)
nu_0 = 2
sigma_0_sq = 1
g = n
# Hyperparameters for the prior
beta_ols = solve(t(X)%*%X)%*%t(X)%*%Y
# MC sampling
S = 10000
#sample sigma_sq
nu_n <- nu_0 + n
Hg <- (g/(g+1))* X%*%solve(t(X)%*%X)%*%t(X)
SSRg <- t(Y)%*%(diag(1,nrow=n) - Hg)%*%Y
nu_n_sigma_n_sq <- nu_0*sigma_0_sq + SSRg
sigma_sq <- 1/rgamma(S,(nu_n/2),(nu_n_sigma_n_sq/2))
#sample beta
mu_n <- g*beta_ols/(g+1)
beta <- matrix(nrow=S,ncol=p)
for(s in 1:S){
Sigma_n <- g*sigma_sq[s]*solve(t(X)%*%X)/(g+1)
beta[s,] <- rmvnorm(1,mu_n,Sigma_n)
}
#posterior summaries
colnames(beta) <- colnames(X)
mean_beta <- apply(beta,2,mean)
round(mean_beta,4)
round(apply(beta,2,function(x) quantile(x,c(0.025,0.975))),4)
sigma_sq_mx <- as.matrix(sigma_sq,nrow=S)
colnames(sigma_sq_mx) <- 'sigma^2'
apply(sigma_sq_mx, 2, function(x) quantile(x,c(0.025,0.975)))
######## Bayesian Model Selection and Averaging
Data_bas <- bas.lm(glu~npreg+bp+skin+bmi+ped+age, data=az, prior="g-prior",alpha=n,
n.models=2^p, update=50, initprobs="Uniform")
plot(Data_bas,which=4)
image(Data_bas)
summary(Data_bas)
coef(Data_bas)
confint(coef(Data_bas))  # confidence intervals
par(mfrow=c(2,2))
plot(coef(Data_bas), subset=2:7,ask=T)
Y <- read.table("http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/swim.dat")
Y <- t(Y)
Y <- read.table("http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/swim.dat")
Y <- t(Y)
# data summaries
n = nrow(Y)
n_swimmers = ncol(Y)
g = n
W = seq(2,12,length.out=n)
X = cbind(rep(1,n),(W-mean(W)))
p = ncol(X)
# Hyperparameters for the prior
nu_0 = 1
sigma_0_sq = 1/10
beta_0 = c(0,0)
n_iter = 1000
# set number of iterations
S <- 10000
BETA = array(0,c(n_swimmers, S, p))
SIGMA_SQ = matrix(0,n_swimmers,S)
for (j in 1:n_swimmers){
beta_ols <- solve(t(X)%*%X)%*%t(X)%*%Y[,j]
# sample sigma_sq
nu_n <- nu_0 + n
Hg <- (g/(g+1))* X%*%solve(t(X)%*%X)%*%t(X)
SSRg <- t(Y[,j])%*%(diag(1,nrow=n) - Hg)%*%Y[,j]
nu_n_sigma_n_sq <- nu_0*sigma_0_sq + SSRg
sigma_sq <- 1/rgamma(S,(nu_n/2),(nu_n_sigma_n_sq/2))
# sample beta
mu_n <- g*beta_ols/(g+1)
beta <- matrix(nrow=S,ncol=p)
for(s in 1:S){
Sigma_n <- g*sigma_sq[s]*solve(t(X)%*%X)/(g+1)
beta[s,] <- rmvnorm(1,mu_n,Sigma_n)
}
BETA[j,,] = beta
SIGMA_SQ[j,] = sigma_sq
}
###### Data
data("hcrabs")
###### Clear environment and load libraries
rm(list = ls())
library(coda)
###### Now to the exercise
#Use a normal proposal
#Choose delta > 0 such that the acceptance probability is very close to 45%
#we will try different values: 0.01, 0.05, 2, 4, 8 and 100
delta <- 0.01
#Initial values for sampler
theta <- 0
#First set number of iterations and burn-in, then set seed
n_iter <- 10000; burn_in <- 0.3*n_iter
set.seed(1234)
#Set counter for acceptances
accept_counter <- 0
#Set null matrices to save samples
THETA <- matrix(0,nrow=n_iter,ncol=1)
#Now, to the Gibbs sampler
for(s in 1:(n_iter+burn_in)){
#generate proposal
theta_star <- rnorm(1,theta, delta)
#compute acceptance ratio/probability
#do so on log scale because r can be numerically unstable
log_r <- log(exp(-0.5*theta_star^2) + 0.5*exp(-0.5*(theta_star-3)^2)) -
log(exp(-0.5*theta^2) + 0.5*exp(-0.5*(theta-3)^2))
if(log(runif(1)) < log_r){
accept_counter <- accept_counter + 1
theta <- theta_star
}
if(s > burn_in){
THETA[(s-burn_in),] <- theta
}
}
#Check acceptance rate
accept_counter/(n_iter+burn_in)
plot(mcmc(THETA))
autocorr.plot(mcmc(THETA))
x <- seq(from=-5,to=7,by=.05)
y <- (2/(3*sqrt(2*pi))) * (exp(-0.5*(x^2)) + 0.5*exp(-0.5*(x-3)^2))
plot(density(THETA),col="red4",lwd=1.5,type="l",xlim=c(-5,7))
points(x,y,col="blue3",xlab=expression(theta),ylab="Density",
main=expression(paste(pi,"(", theta,"|y)")))
labels <- c("True Density", "Accepted Samples")
legend("topright", labels, lwd=2, lty=c(1.5,1.5),
col=c('blue3',"red4"))
###### Clear environment and load libraries
rm(list = ls())
###### Now to the exercise
#Use a normal proposal
#Choose delta > 0 such that the acceptance probability is very close to 45%
#we will try different values: 0.01, 0.05, 2, 4, 8 and 100
delta <- 0.05
#Initial values for sampler
theta <- 0
#First set number of iterations and burn-in, then set seed
n_iter <- 10000; burn_in <- 0.3*n_iter
set.seed(1234)
#Set counter for acceptances
accept_counter <- 0
#Set null matrices to save samples
THETA <- matrix(0,nrow=n_iter,ncol=1)
#Now, to the Gibbs sampler
for(s in 1:(n_iter+burn_in)){
#generate proposal
theta_star <- rnorm(1,theta, delta)
#compute acceptance ratio/probability
#do so on log scale because r can be numerically unstable
log_r <- log(exp(-0.5*theta_star^2) + 0.5*exp(-0.5*(theta_star-3)^2)) -
log(exp(-0.5*theta^2) + 0.5*exp(-0.5*(theta-3)^2))
if(log(runif(1)) < log_r){
accept_counter <- accept_counter + 1
theta <- theta_star
}
if(s > burn_in){
THETA[(s-burn_in),] <- theta
}
}
#Check acceptance rate
accept_counter/(n_iter+burn_in)
plot(mcmc(THETA))
autocorr.plot(mcmc(THETA))
x <- seq(from=-5,to=7,by=.05)
y <- (2/(3*sqrt(2*pi))) * (exp(-0.5*(x^2)) + 0.5*exp(-0.5*(x-3)^2))
plot(density(THETA),col="red4",lwd=1.5,type="l",xlim=c(-5,7))
points(x,y,col="blue3",xlab=expression(theta),ylab="Density",
main=expression(paste(pi,"(", theta,"|y)")))
labels <- c("True Density", "Accepted Samples")
###### Now to the exercise
#Use a normal proposal
#Choose delta > 0 such that the acceptance probability is very close to 45%
#we will try different values: 0.01, 0.05, 2, 4, 8 and 100
delta <- 2
#Initial values for sampler
theta <- 0
set.seed(1234)
#Set counter for acceptances
accept_counter <- 0
#Set null matrices to save samples
THETA <- matrix(0,nrow=n_iter,ncol=1)
#Now, to the Gibbs sampler
for(s in 1:(n_iter+burn_in)){
#generate proposal
theta_star <- rnorm(1,theta, delta)
#compute acceptance ratio/probability
#do so on log scale because r can be numerically unstable
log_r <- log(exp(-0.5*theta_star^2) + 0.5*exp(-0.5*(theta_star-3)^2)) -
log(exp(-0.5*theta^2) + 0.5*exp(-0.5*(theta-3)^2))
if(log(runif(1)) < log_r){
accept_counter <- accept_counter + 1
theta <- theta_star
}
if(s > burn_in){
THETA[(s-burn_in),] <- theta
}
}
#Check acceptance rate
accept_counter/(n_iter+burn_in)
plot(mcmc(THETA))
autocorr.plot(mcmc(THETA))
x <- seq(from=-5,to=7,by=.05)
y <- (2/(3*sqrt(2*pi))) * (exp(-0.5*(x^2)) + 0.5*exp(-0.5*(x-3)^2))
plot(density(THETA),col="red4",lwd=1.5,type="l",xlim=c(-5,7))
points(x,y,col="blue3",xlab=expression(theta),ylab="Density",
main=expression(paste(pi,"(", theta,"|y)")))
labels <- c("True Density", "Accepted Samples")
legend("topright", labels, lwd=2, lty=c(1.5,1.5),
col=c('blue3',"red4"))
###### Clear environment and load libraries
rm(list = ls())
library(coda)
library(rsq)
library(mvtnorm)
install.packages('rsq')
library(rsq)
###### Data
data("hcrabs")
dim(hcrabs)
head(hcrabs)
###### Now to the sampler
#Data summaries
Y <- hcrabs$num.satellites
X <- model.matrix(~color+spine+width+weight,data=hcrabs)
?model.matrix
View(X)
n <- nrow(X)
p <- ncol(X)
#Hyperparameters for the prior
beta_0 <- matrix(0,nrow=p)
Sigma_0 <- diag(1,p)
#Set paramters for proposal density
c <- 0.5
delta <- 0.1 #use it to tune acceptance ratio
var_prop <- delta*var(log(Y+c))*solve(t(X)%*%X)
#Initial values for sampler
beta <- beta_0
#First set number of iterations and burn-in, then set seed
n_iter <- 10000
burn_in <- 0.3*n_iter
thin <- 1
set.seed(1234)
#Set counter for acceptances
accept_counter <- 0
#Set null matrices to save samples
BETA <- matrix(0,nrow=n_iter,ncol=p)
#Now, to the Gibbs sampler
for(s in 1:(n_iter+burn_in)){
#generate proposal
beta_star <- t(rmvnorm(1,beta,var_prop))
#compute acceptance ratio/probability
#do so on log scale because r can be numerically unstable
log_r <- sum(dpois(Y,exp(X%*%beta_star),log=T)) + dmvnorm(c(beta_star),beta_0,Sigma_0,log=T) -
sum(dpois(Y,exp(X%*%beta),log=T)) - dmvnorm(c(beta),beta_0,Sigma_0,log=T)
if(log(runif(1)) < log_r){
accept_counter <- accept_counter + 1
beta <- beta_star
}
if(s > burn_in){
BETA[(s-burn_in),] <- beta
}
}
#Check acceptance rate
accept_counter/(n_iter+burn_in)
#thinning
sample_thin <- seq(1,n_iter,by=thin)
BETA_thinned <- BETA[sample_thin,]
colnames(BETA_thinned) <- colnames(X)
plot(mcmc(BETA_thinned))
#unlike Gibbs sampling, we do have some stickiness when doing Metropolis/M-H
#you also saw this in the lab
apply(BETA_thinned,2,effectiveSize)
#trace plots look fine
autocorr.plot(mcmc(BETA_thinned))
#quick convergence diagnostics.
geweke.diag(mcmc(BETA_thinned))
?geweke.diag
#posterior summaries
round(apply(BETA_thinned,2,mean),2)
round(apply(BETA_thinned,2,function(x) quantile(x,probs=c(0.025,0.975))),2)
#Just for reference, take a look at freqentist model
freq_model <- glm(num.satellites~color+spine+width+weight,family=poisson,data=hcrabs)
summary(freq_model)
